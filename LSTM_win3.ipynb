{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-win3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rokia88/EMV/blob/master/LSTM_win3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfsv0MhDWVKz"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from pprint import pprint\n",
        "from urllib.parse import unquote\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "import re\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from keras.utils import plot_model,to_categorical\n",
        "# Scikit learn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "# Word2vec\n",
        "import gensim\n",
        "import multiprocessing\n",
        "# Keras\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.constraints import maxnorm\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint,EarlyStopping\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import keras_metrics\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "checkpoint_filepath = './checkmodel5/'\n",
        "datasetfile = './csic3.csv'\n",
        "word2vec_filepath = './word2vec1.model'\n",
        "sgd = SGD(lr=0.01,momentum=0.9,decay=0.003, nesterov=True)\n",
        "adam = Adam()\n",
        "OPTIMIZER = adam\n",
        "BATCH_SIZE = 128\n",
        "NB_EPOCH = 10\n",
        "# load the dataset\n",
        "def load_dataset(filename):\n",
        "    # load the dataset as a pandas DataFrame\n",
        "    dataset = read_csv(filename,names=['URL','label'], sep=';',header=0)\n",
        "    # split into input (X) and output (y) variables\n",
        "    X = dataset\n",
        "    y = dataset[['label']]\n",
        "    print(X.shape)\n",
        "    print(y.shape)\n",
        "    return X, y\n",
        "\n",
        "def URLDECODE(payload):\n",
        "    payload = payload.lower()\n",
        "    while True:\n",
        "        test = payload\n",
        "        payload = unquote(payload)\n",
        "        if test == payload:\n",
        "            break\n",
        "        else:\n",
        "            continue\n",
        "    payload, num = re.subn(r'\\d+', \"0\", payload)\n",
        "    \n",
        "    payload, num = re.subn(r'(http|https)://[a-zA-Z0-9\\.@&/#!#\\?]+', \"http://u\", payload)\n",
        "    \n",
        "    r = '''\n",
        "        (?x)[\\w\\.]+?\\(\n",
        "        |\\)\n",
        "        |\"\\w+?\"\n",
        "        |'\\w+?'\n",
        "        |http://\\w\n",
        "        |</\\w+>\n",
        "        |<\\w+>\n",
        "        |<\\w+\n",
        "        |\\w+=\n",
        "        |>\n",
        "        |[\\w\\.]+\n",
        "        '''\n",
        "    return nltk.regexp_tokenize(payload, r)\n",
        "\n",
        "# init callback class\n",
        "class callback(CallbackAny2Vec):\n",
        "    \"\"\"\n",
        "        Callback to print loss after each epoch\n",
        "        \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "    \n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "        if self.epoch == 0:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "        else:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "        self.epoch += 1\n",
        "        self.loss_previous_step = loss\n",
        "\n",
        "def listToString(s): \n",
        "    # initialize an empty string\n",
        "    str1 = \"\"\n",
        "    # traverse in the string\n",
        "    for ele in s:\n",
        "        str1 += ele\n",
        "    # return string\n",
        "    return str1\n",
        "\n",
        "def create_model(embedding_matrix,vocab_size,MAX_LEN):\n",
        "    # define the model\n",
        "    # input layer\n",
        "    initializer = tf.keras.initializers.HeNormal()\n",
        "    input = tf.keras.Input(shape=(MAX_LEN,))\n",
        "    embd =  tf.keras.layers.Embedding(vocab_size,128,weights=[embedding_matrix],input_length=MAX_LEN,trainable=False)(input)\n",
        "    lstm1 = tf.compat.v1.keras.layers.CuDNNLSTM(128,kernel_initializer=initializer)(embd)\n",
        "    lstm1 = tf.keras.layers.LayerNormalization()(lstm1)\n",
        "    lstm2= tf.compat.v1.keras.layers.CuDNNLSTM(256,kernel_initializer=initializer)(embd)\n",
        "    lstm2 = tf.keras.layers.LayerNormalization()(lstm2)\n",
        "    lstm3= tf.compat.v1.keras.layers.CuDNNLSTM(512,kernel_initializer=initializer)(embd)\n",
        "    lstm3 = tf.keras.layers.LayerNormalization()(lstm3)\n",
        "    merge = tf.keras.layers.concatenate([lstm1,lstm2,lstm3])\n",
        "    merge = tf.keras.layers.LayerNormalization()(merge)\n",
        "    merge = tf.keras.layers.Dropout(0.5)(merge)\n",
        "    hidden4 = tf.keras.layers.Dense(1024, activation='tanh',kernel_initializer=initializer)(merge)\n",
        "    hidden4 = tf.keras.layers.LayerNormalization()(hidden4)\n",
        "    hidden4 = tf.keras.layers.Dropout(0.5)(hidden4)\n",
        "    output = tf.keras.layers.Dense(2, activation='softmax')(hidden4)\n",
        "    model = tf.keras.Model(inputs=[input], outputs=output)\n",
        "    \n",
        "    # summarize the model\n",
        "    print(model.summary())\n",
        "    tf.keras.utils.plot_model(model, to_file='shared_input_layer.png')\n",
        "    # compile the model\n",
        "    model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def prepare_targets(y_train, y_val, y_test):\n",
        "    le = LabelEncoder()\n",
        "    le.fit(y_train)\n",
        "    y_train_enc = le.transform(y_train)\n",
        "    y_test_enc1 = le.transform(y_test)\n",
        "    y_val_enc = le.transform(y_val)\n",
        "    y_train_enc = to_categorical(y_train_enc, num_classes=2)\n",
        "    y_test_enc = to_categorical(y_test_enc1, num_classes=2)\n",
        "    y_val_enc = to_categorical(y_val_enc, num_classes=2)\n",
        "    return y_train_enc, y_val_enc, y_test_enc,y_test_enc1\n",
        "\n",
        "def texts_to_sequences(s_tokens,word_index):\n",
        "    result = []\n",
        "    l2 = []\n",
        "    for s in s_tokens:\n",
        "        for token in s:\n",
        "            if word_index.get(token) is not None:\n",
        "                l2.append(word_index.get(token))\n",
        "            else:\n",
        "                l2.append(0)\n",
        "        result.append(l2)\n",
        "        l2 = []\n",
        "    print(result[0],result[1])\n",
        "    if(len(result)==0):\n",
        "        print(\"result empty\")\n",
        "    return result\n",
        "\n",
        "def standardize_data(x_train,x_test,x_val):\n",
        "    trans = StandardScaler()\n",
        "    trans.fit(x_train)\n",
        "    x_train_std = trans.transform(x_train)\n",
        "    x_test_std = trans.transform(x_test)\n",
        "    x_val_std = trans.transform(x_val)\n",
        "    return x_train_std,x_test_std,x_val_std\n",
        "\n",
        "def find_max_list(list):\n",
        "    list_len = [len(i) for i in list]\n",
        "    return (max(list_len))\n",
        "\n",
        "X, y = load_dataset(datasetfile)\n",
        "run = 0\n",
        "loss_avg = 0\n",
        "acc_avg = 0\n",
        "prec_avg = 0\n",
        "reca_avg = 0\n",
        "acc_min = 1\n",
        "acc_max = 0\n",
        "MAX_RUN = 5\n",
        "while run < MAX_RUN:\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.33,stratify=y,shuffle= True,random_state=20)\n",
        "        # prepare output data\n",
        "        #y_train_enc1, y_test_enc= prepare_targets(y_train1, y_test)\n",
        "        X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.16,stratify=y_temp,shuffle= True,random_state=20)\n",
        "        # prepare output data\n",
        "        y_train_enc, y_val_enc, y_test_enc,y_test_enc1= prepare_targets(y_train, y_val,y_test)\n",
        "        print(X_train.head())\n",
        "        print(X_train.shape)\n",
        "        print(X_train.label.value_counts())\n",
        "        print(X_test.head())\n",
        "        print(X_test.shape)\n",
        "        print(X_test.label.value_counts())\n",
        "        print(X_val.head())\n",
        "        print(X_val.shape)\n",
        "        print(X_val.label.value_counts())\n",
        "        # split text in tokens\n",
        "        # split text in tokens\n",
        "        sentences = [URLDECODE(listToString(text)) for text in X.values[:,:-1]]\n",
        "        model = gensim.models.Word2Vec(alpha=0.1,min_alpha=0.0007,min_count=1, window=3,size=128)\n",
        "        model.build_vocab(sentences)\n",
        "        model.train(sentences, total_examples=model.corpus_count, epochs=30,compute_loss = True,callbacks=[callback()])\n",
        "        #model = gensim.models.Word2Vec.load(word2vec_filepath)\n",
        "        words = list(model.wv.vocab)\n",
        "        print(\"Vocabulary size: %i\" % len(words))\n",
        "        sentences_training = []\n",
        "        for text in X_train.values[:,:-1]:\n",
        "            l1 = URLDECODE(listToString(text))\n",
        "            for u in l1:\n",
        "                sentences_training.append(u)\n",
        "        # creating a dictionary of vocab with index\n",
        "        vocab = sorted(set(sentences_training))\n",
        "        vocab = list(enumerate(vocab, 1))\n",
        "        print(len(vocab))\n",
        "        # putting the index first\n",
        "        indexed_vocab = {k:v for v,k in dict(vocab).items()}\n",
        "        print(len(indexed_vocab))\n",
        "        print(\"Create Embedding matrix\")\n",
        "        #word_index = tokenizer.word_index\n",
        "        vocab_size = len(words) + 1\n",
        "        embedding_matrix = np.zeros((len(words) + 1, 128))\n",
        "        for word, idx in indexed_vocab.items():\n",
        "            if word in words:\n",
        "                embedding_vector = model.wv.get_vector(word)\n",
        "                if embedding_vector is not None:\n",
        "                    embedding_matrix[idx] = model.wv[word]\n",
        "        print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))\n",
        "        print(\"Build Keras model\")\n",
        "        sentences_train = [URLDECODE(listToString(text)) for text in X_train.values[:,:-1]]\n",
        "        x_train= keras.preprocessing.sequence.pad_sequences(texts_to_sequences(sentences_train,indexed_vocab),maxlen=20)\n",
        "        sentences_val = [URLDECODE(listToString(text)) for text in X_val.values[:,:-1]]\n",
        "        x_val = keras.preprocessing.sequence.pad_sequences(texts_to_sequences(sentences_val,indexed_vocab),maxlen=20)\n",
        "        sentences_test = [URLDECODE(listToString(text)) for text in X_test.values[:,:-1]]\n",
        "        x_test = keras.preprocessing.sequence.pad_sequences(texts_to_sequences(sentences_test,indexed_vocab),maxlen=20)\n",
        "        print('x_train shape: %s' % str(x_train.shape))\n",
        "        print(x_train[0,:])\n",
        "        print('x_val shape: %s' % str(x_val.shape))\n",
        "        print(x_val[0,:])\n",
        "        print('x_test shape: %s' % str(x_test.shape))\n",
        "        print(x_test[0,:])\n",
        "        checkpoint_filepath = checkpoint_filepath+str(run)+\"cp5.ckpt\"\n",
        "        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy',\n",
        "                                                    mode='auto',save_best_only='True',save_weights_only='True',verbose=1)\n",
        "        #early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, mode='max')\n",
        "        #rop = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='max')\n",
        "        #scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
        "        #callbacks = [early_stopping, rop]\n",
        "        print(np.any(np.isnan(x_train)))\n",
        "        print(np.any(np.isnan(y_train_enc)))\n",
        "        print(np.any(np.isnan(x_val)))\n",
        "        print(np.any(np.isnan(y_val_enc)))\n",
        "        print(np.any(np.isnan(x_test)))\n",
        "        print(np.any(np.isnan(y_test_enc)))\n",
        "        #trainng = DataFrame(x_train)\n",
        "        #print(trainng.describe())\n",
        "        #x_train,x_test,x_val = standardize_data(x_train,x_test,x_val)\n",
        "        #trainng_2 = DataFrame(x_train)\n",
        "        #print(trainng_2.describe())\n",
        "        MAX_LEN = 20\n",
        "        k_model = create_model(embedding_matrix,vocab_size,MAX_LEN)\n",
        "        history = k_model.fit(x=x_train,verbose=2,\n",
        "                y=y_train_enc,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                epochs=NB_EPOCH,\n",
        "                validation_data=(x_val, y_val_enc),\n",
        "                callbacks=[checkpoint])\n",
        "        pyplot.subplot(211)\n",
        "        pyplot.title('Loss')\n",
        "        pyplot.plot(history.history['loss'], label='train'+str(run))\n",
        "        pyplot.plot(history.history['val_loss'], label='test'+str(run))\n",
        "        # plot accuracy during training\n",
        "        pyplot.subplot(212)\n",
        "        pyplot.title('Accuracy')\n",
        "        pyplot.plot(history.history['accuracy'], label='train'+str(run))\n",
        "        pyplot.plot(history.history['val_accuracy'], label='test'+str(run))\n",
        "        new_model = create_model(embedding_matrix,vocab_size,MAX_LEN)\n",
        "        new_model.load_weights(checkpoint_filepath)\n",
        "        loss, accuracy = new_model.evaluate(x_test,y_test_enc, verbose=0)\n",
        "        print('RUN:%f'%(run))\n",
        "        predicted_classes = new_model.predict(x_test)\n",
        "        print(predicted_classes)\n",
        "        yhat = np.argmax(predicted_classes, axis=1)\n",
        "        print(classification_report(y_test_enc1, yhat))\n",
        "        print('Accuracy: %f, loss:%f' % (accuracy*100,loss))\n",
        "        acc_avg += accuracy\n",
        "        loss_avg += loss\n",
        "        if (accuracy<acc_min):\n",
        "            acc_min = accuracy\n",
        "        if (accuracy>acc_max):\n",
        "            acc_max= accuracy\n",
        "        run = run +1\n",
        "acc_avg = acc_avg/MAX_RUN\n",
        "loss_avg = loss_avg/MAX_RUN\n",
        "print('Max Accuracy: %f,Min Accuracy: %f,Avg Accuracy: %f, Avg loss:%f' % (acc_max*100,acc_min*100,acc_avg*100,loss_avg))\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzxVhFIxmM6i"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install keras-metrics\n",
        "!cp \"drive/My Drive/Colab Notebooks/csic3.csv.zip\" .\n",
        "# importing required modules \n",
        "from zipfile import ZipFile \n",
        "\n",
        "# specifying the zip file name \n",
        "file_name = \"csic3.csv.zip\"\n",
        "\n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "  # printing all the contents of the zip file \n",
        "  zip.printdir() \n",
        "\n",
        "  # extracting all the files \n",
        "  print('Extracting all the files now...') \n",
        "  zip.extractall() \n",
        "  print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlzOaNEimGH7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}